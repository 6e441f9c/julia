features
* f(x) = x syntax
* write up module design ideas
* switch open type parameters to TypeVars
- next_method
- common_supertype
- improve the situation with numeric literals somehow

-------------------------------------------------------------------------------

speculation
- complex number syntax?
* declaring abstract types
- a type with plain-old-data fields can be treated as a value type if it
  is immutable. a mutable value type is just a thing that's allowed to behave
  confusingly. the immutability is key, "value type" is an optimization
  the user doesn't have to worry about.
  to handle immutable-element-in-mutable-container, we need a special meaning
  for "a[i].re = x". it means a[i] = complex(x, a[i].im)...
  most plausible implementation is to translate it to
  a[i] = setfield(a[i], 're, x)
  and have setfield copy value types

- recursive types (tree.j) are too verbose. we should be able to say something
  like:
  type EmptyTree = ()
  type Tree[`T] = Union[EmptyTree, (data=T,left=Tree[T],right=Tree[T])]

syntax idea:

typeunion Tree[T]
  struct EmptyTree
  end

  struct TreeNode[T]
    data::T
    left::Tree[T]
    right::Tree[T]
  end
end

Much better way to do recursive types without fiddly syntax!!
Means the exact same thing as what we have now in tree.j with Union(),
except permits the required nesting!

- maybe make everything invariant, use where-clauses instead
  where-clauses create a constraint environment, sufficient to
  express "field types". For example
  Array[dims, data] where dims<:Buffer[Size], data<:Buffer

* explicit different kinds of types. e.g. StructType, NumericType
  . conceptually helpful, because you can only use new(T,...) on struct types.
    new(Int8) doesn't work, and new(Symbol) doesn't work...
* use {} for type parameters, e.g. Complex{Float64}

- idea: only allow field access (dot) on variables whose type is statically
  known. this (1) guarantees that field access is fast, and (2) allows us
  to prevent field access on types not visible (exported) to the current
  module. Example

module A
export Public
struct Public
  field::Private
end
struct Private
  a
  b
end
end

module B
import A.Public
x = Public.new()  # ok
x.field           # ok
x.field.a         # not allowed
end

Another way to see this is: does it makes sense to write "f(x) = x.a"? In
other words field access is not a generic operation. We have 3 kinds of
operations:
static: like add_int32, requires specific statically-known types
function: like the functions is() and istype(), works on any type or almost
          any type, but may not have new definitions added.
generic function: the most general kind of function. supports dispatch.

the question is, which kind of operation is field access?
currently it is kind #2.

- want some way to define types that are variants of others, for example
  matrices with named rows and columns, without lots of redundant work.

-------------------------------------------------------------------------------

a perpetual question is "where do objects come from?"
there are two views: user-defined constructors (C++,java,etc.), and builtin
functions (C,scheme,matlab,etc.)

constructor view:
. there is a hook to run code every time a type is instantiated
. you can have uninitialized instances
. the "actual" allocation routine is hidden, and the special form "new" is
  used. "new T()" means roughly
    x = magic_allocate<T>()
    call T's constructor with this==x
    return x
. constructor is "inside" the type, so type parameters naturally and easily
  parameterize the constructor as well.

function view:
. the magic_allocate function is exposed, anybody can make any object
. our new(T,...) forces you to provide values of correct types for all
  fields, eliminating null references ("RAII")
. consistent with our design of not encapsulating functions with types
. BUT no way to enforce invariants other than field types. for example
  you can make an Array where the dims is the wrong length!

C++-style constructor is actually an initializer; it doesn't return a value.

if we have constructors that return values, they might return nonsense.

if not, we need special syntax like new, and to magically deliver the
actual object to the constructor. but how to enforce that it gets
initialized? do we require DFA to show that all fields are set and none
are accessed before being set? this constrains the programming style.

IDEA: have a special new() function in scope for user-defined constructors
inside the type definition:

struct Foo
    ...

    function Foo(any, arguments, x)
        return new(any-x, x*any)
    end
end

new() is the only way to make objects. it's a sealed generic
function that only accepts arguments of the field types, and it's only
visible inside the struct definition, so you can guarantee that immutable
objects obey your invariants. Then calling Foo() externally calls
your constructors. You can define new Foo() methods externally, but they
can't see new() so they have to call other Foo constructors. You get full
flexibility plus an abstraction barrier. If you don't write any constructors
you get the default, Foo(args...) = new(args...).

-------------------------------------------------------------------------------

conversion/dispatch test cases:

1+rational(1,2)
rational(1,2)+0.5
1./complex(2.,2.)
1/complex(2.,2.)

-------------------------------------------------------------------------------

notes from "Using category theory to design implicit conversions and
generic operators"

conversions must commute:
OP(convert(x,T),convert(y,T)) == convert(OP(x,y),T)
if A->B and B->C, then A->C must be the composition of these two conversions

define operators for "key sorts", like (int,int) (real,real) and let
conversions handle the rest (this was our idea too!)

"one can never use the same operator for the equality relation on different
data types when the data types are connected by an implicit conversion
function which is not an injection"

-------------------------------------------------------------------------------

functions needed to port colorimetry library:

construct matrix,vector from scalars
persistent constant matrices
1d,2d scalar indexing
2d indexing with colon
elementwise mat,vec ops
matrix multiply
dot product
min,max
transpose
1d comprehension
-------------------------------------------------------------------------------

invariance vs. covariance

what does Array[Number] mean in various contexts?

new(Array[Number,n],...)
make an array that can hold various kinds of numbers, but only numbers

as a typed location, e.g. "local x::Array[Number]"
doesn't really matter. both the user & compiler don't know exactly
what kind of numbers will come out of there. user doesn't care whether
it is actually an Array[Float64], etc., but compiler does.

matching argument types, foo(x::Array[Number])
would you want to write a function just for arrays created with element
type number? sometimes a function works differently on cell arrays vs.
numeric arrays. do you need to ensure that foo(x::Array[Any]) is only
called on an actual cell array, or would it be ok to substitute any
sort of array?

with invariance, you can express this. only actual cell arrays will match.
otherwise you can write Array[T <: Any], Array[T <: Number], etc.
under covariance, the compiler effectively treats Array[Any] as Array[T]
because it needs to specialize it anyway.

-------------------------------------------------------------------------------

paper/thesis ideas:

design recommendations for technical computing languages
why use dynamic languages for technical computing

the design and implementation of julia

reinventing the repl: language support for interactive use
propagating assumption violations in jit compiled systems

improving numeric type systems, e.g. approach to type promotion

a highly unobtrusive type system
separating ad-hoc from parametric polymorphism heuristically

eliminating array allocations in loops
or using register allocation to manage general resources
or inference for minimizing temporary space use. e.g. figure out
  that in-place matmul can be done with O(n) space overhead.

performance survey over the course of a language implementation (with llvm)

destination-passing (generalized nargout), allowing discovery of some
  aspects of the continuation to get some of the benefits of lazy
  evaluation or for space optimization.

comprehension optimizations
extensible auto-vectorizing

-------------------------------------------------------------------------------

function representation
-----------------------

new functions are created in many ways:
- new (empty) generic function
- copy closure
- adding a template method to a generic function
  . any function can have static parameters, but automatic
    instantiation based on argument types is done by generic functions
- creating a new specialization inside a generic function.
  . type-specific (non-template) method is added manually by the user,
    compiled on first call
  . a more general method matches, compile for new types
  . a template method matches; apply static lambda and compile
  . *** We need a way to create the generic function and cache all
    generated code for an inner function only once, but still be able to
    add new closure environments.

  important optimization: sharing generated code for generic inner functions
  . must lift generic function creation to the top level, then when
    we enter the enclosing function, shallow copy the GF and replace
    all the closure environments with new ones. hard case is when an
    inner function has multiple definitions, each one needs a different
    closure environment. or we could combine all definitions for
    inner functions into a single one (since we can see the full set of
    definitions statically), and have a special inner-generic-function
    that can share an entire method table. on each call to the encl.
    function we just make a new IGF object with a new cloenv, and the IGF
    passes this cloenv onto any method that's actually invoked.

  . for now we will just have to make a new GF on each call to the enclosing
    function

closure ::=
    <
    type tag - a function type
    code pointer
    closure data
    static data
    >

new_closure copies this, with new closure data field

static data ::=
    <
    name
    AST
    static parameter env
    >


a GF looks like <Any-->Any, apply_generic, (methtable,), ()>

in the "let over lambda" case, a top-level GF method might have a closure
environment.

(lambda args (var-info (locals ...) vinfo-list capt-list static-params)
             #(body statements))

so in the interpreter the static parameters are inside the AST, but no
big deal.

-------------------------------------------------------------------------------

issues 5/31/10

- unions, tagged unions
  consider Cons(Cons(`a,nil), Cons(nil, nil)), or {{a}, {}}
  the type of {{}} is inferred as Cons{EmptyList}, then we have
  Cons{Cons{Symbol}}. these need to match. if we picked
  T == List{Symbol} == Union(EmptyList,Cons{Symbol}), it would have worked.
  Should there be both Cons{Cons{Symbol}} and Cons{List{Symbol}} ?
  . the way it would work is during matching, T starts as Cons{EmptyList}.
    when we try to add T==Cons{Cons{Symbol}}, we see EmptyList and Cons
    are disjoint, but they explicitly indicate a common supertype List,
    so we can pick that. type_match will only pick values of T from the
    types actually present, or their typeunion parents.

- inferred type parameters. problem with union types, e.g.
  struct Foo{T,S}
    a::Nullable{T}
    b::Nullable{S}
  end

  what is the type of Foo((),()) ?
  Should we give it unconstrained type variables, then adjust their bounds
  when the object is mutated?
  It would be great to get rid of T{...}.new(...) altogether.
  . idea is to use Nullable{T}==Union(T,Null{T}), but currently that Union
    type is too complex!

* in f{T}(T, T) allow the actual types to differ as long as one is a
  supertype of the other. for example, taking the type intersection of
  (Real, Int) with (T, T) should give (T<:Int, T<:Int)

* in matching_methods, don't include signature S if we've already found
  a signature R such that T <: R <: S, where T is the type we're searching for

* given f(Int32, Int32), matching f(T, T) and giving it priority over
  f(Int, Int) is wrong. typevars should be treated like types, and all
  methods must be consistently comparable with each other for specificity.
  . monotonicity: if the declaration of A is more specific than B,
    it remains that way for all possible actual arguments.

* need to make Type <: Function
  plus SomeType{T}(args...) should work, no .new
  make structs with no fields singletons
  so both Complex and Complex{...} should be types, and also applicable

plan:
* TypeConstructor only for typealias
* typename->ctor becomes typename->primary, and points to the original
  version of a type
* Complex, e.g., is a StructType. both Function and StructType are applicable
  . this avoids the issue of abstract types being functions, which is crazy
* T{...} can work on either TypeConstructor or a TagType
* when applying a type or typector to concrete types, enforce that all typevar
  bounds are respected.
* start off every struct type as a constructor-factory-trampoline function,
  so no constructors are created until constructing is actually attempted.
* add the ability to seal a generic function. use for constructors.
. keep track of whether a method is "generic" explicitly, instead of
  using jl_has_typevars. when matching, make sure the method's own static
  parameters were what matched, e.g.
  foo{T,S}(a::SomeType, b::OtherType)
  typevars in SomeType and OtherType might match, but we could not infer T,S
  so this is an error. this fully generalizes the "cannot infer type
  parameters from arguments" error in generic constructors, so it would not
  be needed there any more.
. consider changing the rules so in f(x::T...) T is the union of the actual
  argument types.

* add the ability to hide the default constructor, which also makes it
  possible to add multiple constructor overloads:

struct Rational <: Real
    num::Int
    den::Int

    #implicit: new(x::Int, y::Int) = construct(Rational, x, y)

    Rational(n::Int, d::Int) = new(lowest_terms(n,d)...)
    Rational(n::Int) = Rational(n, 1)
end

or if no constructor defs are present:
   add_generic_constructor(Rational), which does
   Rational(x::Int, y::Int) = construct(Rational, x, y)

struct Rational{T<:Int} <: Real
    num::T
    den::T

    #implicit: new{T<:Int}(x::T, y::T) = construct(Rational{T}, x, y)

    new::((T,T)-->Rational{T})
    Rational(n::Int, d::Int) = new(lowest_terms(n,d)...)
    Rational(n::Int) = Rational(n, 1)
end

   add_generic_constructor(Rational) does
   Rational{T<:Int}(x::T, y::T) = construct(Rational{T}, x, y)

now what does e.g. Rational{Int8} mean?
technically we should repeat the whole declaration with this substitution
done:
struct Rational{Int8} <: Real
    num::Int8
    den::Int8

    #implicit: new(x::Int8, y::Int8) = construct(Rational{Int8}, x, y)

    Rational(n::Int, d::Int) = new(lowest_terms(n,d)...)
    Rational(n::Int) = Rational(n, 1)
end

the key is to keep the closure that defines the constructors:

constructor_defining_closure(Rational, new) = begin
   ... all user's constructor decls go here ...
end

initially we call constructor_defining_closure(Rational, new)

when we instantiate a type, e.g. Rational{Int8}, we make a new type
object that's also an empty generic function (no methods).
then we make "new", the base constructor for it.
R = instantiate(Rational,(Int8,))
new = jl_new_closure(jl_new_struct_internal, R)
R.constructor_defining_closure(R, new)


struct Foo
  ...
  Foo(x) = new(x, 0)
end

struct MySingleton
  ...
  instance = ()
  MySingleton() = is(instance,()) ? (instance=new(...)) : instance
end

if no ctor definitions preset, you get Foo(...) = new(...)

-------------------------------------------------------------------------------

idea: fault-tolerant remote references
you do spawn(thunk), and it runs somewhere. saving the thunk might allow
the same value to be recomputed later. when the work is done, we time it.
if it takes longer to compute than some estimate of how long it takes to
send, we can replicate it somewhere. the caller gets a remote ref back
with a list of locations where the value can be found. if they all fail
the value can be recomputed.

can a program written in terms of remote references be converted to SPMD?

-------------------------------------------------------------------------------

issues 6/21/10

- inference within closures
  * avoid box for variables (arguments) that aren't assigned
    . even better, for variables that are assigned definitely and once
  . then multiple assignments in the binding scope but not in closures
  . then assignments within closures
* inference for new constructor stuff
* hooking up the pieces to use inferred types in the compiler
  . we could save tuples (type_sig, AST) as s-exprs to avoid the cost
    of lowering and type inference.
* limit specialization of vararg functions
* method cache invalidation, also invalidate all caches once type inference
  is available (after loading start.j)
? don't specialize on arguments declared as Any. make the default type
  an unconstrained typevar, and specialize those.

* coroutines, Tasks, exceptions
* let
* ans
* Char type, more flexible bits types
* make pmap_d work
* bitstype syntax
* boot.j
- calling convention optimization, w/ wrapper funcs for generic calling
* intrinsic calls from command line (use compiler)
? new for loop, comprehension, reduction formulation
* faster I/O (not byte at a time!)
- expose word size in the language?
  . right now you can't define Ptr
  . make Size the same as size_t

* try/catch
- flatten memory layout for Array of Complex (immutability)

* "let i=0; print(i); end" should parse. use commas for init exprs
* parsing hex 0x...
* '"'
* repl global scope change
- clear
* preparsed source
* type intersect bug: Array ∩ Scalar
* make global decls inherited
- ccall calls convert
* << >> precedence
* break in value position
* tintersect((Int,Int),(T,T)) is wrong when T is T<:Int, ok when T<:Any
* use nothing instead of () where appropriate

method ordering tests:
//:  (T,T) before (Any,Any)
+:   (T<:Number{T},T<:Number{T}) before (Number{T},Number{T})
promote_type: promote_type(Type{T},Type{T}) first
ref: ref(Array{Any,1},Int32) before ref(Array{T,1},Int32)

convert files for new GC:
*alloc.c    *codegen.cpp   *gf.c      *intrinsics.cpp
*ast.c      *init.c        *io.c      *task.c
*builtins.c *interpreter.c *jltypes.c *repl.c

startup time, mem0, mem after tests.j, exe size:
marksweep: 7.8s, 58356  43m, 69792  55m, 8285616
 eagerfree:7.8s, 49784  34m  65452  51m
  no inf:  2.1s, 35508  20m
boehm:     7.8s, 50236  35m, 78320  63m, 8376416

-------------------------------------------------------------------------------

issues 11/6/10

* faster GC stack roots
* fix A[I..., end]
* make Tasks exit on exceptions (not ready for continuable-exc yet)
* quote syntax change
- implement longjmp block exits so return/break work across functions
* splicing tuples in ` with $...
* readall, readline

* statically evaluate apply_type when possible
* optimize Array constructor (avoid checks and apply_type)
arr{T,N}(::Type{T}, d::NTuple{N,Size}) = Array{T,N}(d)
arr{T}(::Type{T}, d::Size...)          = arr(T, d)
arr{T}(::Type{T}, m::Size,n::Size)     = Array{T,2}((m,n))
julia> tic(); for i=1:100000; a=Array(Int32,2,3); end; toc()
elapsed time: 0.0483038425445557 sec
elapsed time: 0.0244669914245605 sec
elapsed time: 0.0549869537353516 sec
6/2/11: ~0.008 sec
6/3/11:  0.0068681240081787 sec
the next level of sophistication would be something like this:
function array_ctor_factory(T, N)
    eval(
    quote
        let Array
            function Array(d::NTuple{$N,Size})
                ccall(:jl_new_array,Any,(Any,Any),Array{$T,$N},d)::Array{$T,$N}
            end
        end
    end)
end
which could let us avoid the type checks in jl_new_array_internal.
BUT we get the same benefit just by using Array() instead of Array{}().

-------------------------------------------------------------------------------

issues 12/21/10

- inference in inner functions
* allow anything iterable to be spliced by ...
* more flexible string literals
* something about uninitialized objects in constructors
  old news (for reference):
  consider generating code for constructors, e.g.
  function new{T}(re::T, im::T)
    v = newstruct(Complex{T})
    v.re = re; v.im = im
    v
  end
  . can be specialized, allows removing checks and converts
  or have uninitialized objects passed in, e.g.
  Range1{T}(r::Range1{T}, start, stop) = (r.start=start;r.stop=stop;r)
  Range1{T}(start::T, stop::T) = Range1{T}(start, stop)

  every type MyType with no parameters gets this defined automatically:
  MyType(args...) = MyType(new_internal(MyType), args...)
  and you need to define MyType(m::MyType, ...)
  if MyType has parameters, then instantiated versions behave that way.
  So Complex{Int32} would have just the single method
      function (args...) = Complex(new_internal(Complex{Int32}), args...)
  and you would define Complex(z::Complex, ...)
  you can also define e.g. Complex{T}(r::T) = Complex{T}(r, 0)

  actual new design:
 struct Rational{T<:Int} <: Real
     num::T
     den::T
 
     Rational(n::Int, d::Int) = (g=gcd(n,d);
                                 # can use T here
                                 this.num=div(n,g); this.den=div(d,g))
 end
 Rational{T}(n::T, d::T) = Rational{T}(n,d)
 Rational(n::Int, d::Int) = Rational(promote(n,d)...)
 Rational(n::Int) = Rational(n,1)

default ctors would be
  inside type block: Rational(n, d) = (this.num=n;this.den=d)
  outside:           Rational{T}(n::T, d::T) = Rational{T}(n,d)

  * frontend changes
  * convert all .j to new constructors
  * instantiate constructor factory with type's static parameters, and
    type's name bound to the instantiated type
  * implement (new ...) in codegen
    . can be optimized more
  * (new ...) case in inference
  * null check in field access
  * rename clone

* type=>abstract, struct=>type
* do something about "global X+=1"
? make sure chained comparisons evaluate left to right
* +(a,b) should be a 2-argument call, not unary op of a tuple
* 2(...) as multiplication

before removing the 1-word overhead:
initial   , after tests.j
51524  36m, 68484  53m
after:
48564  33m, 62844  48m

-------------------------------------------------------------------------------

issues 1/24/11

* add dequeue functionality to 1-d arrays
    new array axioms:
    array_grow_end, array_del_end, array_grow_beg, array_del_beg,
    array_size(a, n), array_size(a)
- array constructors including initialization
  Array(Int32, (4,),  0, 1, 2, 3)
  Array(Int32, (2,2), 0, 1, 2, 3)
  Array{Int32,1}(5,    0, 1, 2, 3, 4)
  Array{Int32,2}(2,2,  0, 1, 2, 3)
- keyword args
  f(x; a=1, b=0)
  f(x; kws...)
  . add nkwargs to calling convention
  . add callkw AST node. pass keywords as alternating symbol/value, but
    callkw sets nkwargs to something nonzero.
  . insert code to handle kw arg assignment, default values
  . insert code to convert unused kw args to some object if requested
  . f(x, b...; a=v, k...) does (call applykw 1 f (x,) b :a v k)
- modules
* saving system image
* make optimized arrayset, arrayref work on all Arrays
- parallel
  * make sure process group setup works
  * function reflection and serialization
  * automatically resolve local remote references to their values
  * @spawn
  * distributed GC
  . uninitialized darrays that wait for assignment
  . be able to wait for more general conditions on shared objects, e.g.
    a channel could be a GlobalObject on 2 processors, and you wait for a
    message on the channel. or a DArray might be updated multiple times, and
    you want to wait for the next write (not just the first)
  ? some kind of atomic update, e.g. a[a:b,c:d]=X where once the write begins
    future reads from that region will wait for it to complete
  ? isready()
  . timer events, delay and aggregate messages
  * lambda-numbering to reuse code for spawned closures
  * pull repl global variables into spawned closures
  . do indexing of local objects locally automatically in spawn
* add line number and file information
- keep track of renamed var names

? try to remove @ from macro syntax
* either add "do" to loops or use
  @parallel for i=1:n
    ...
  end
? clean up some type system stuff:
  . remove tvar->bound, methlist->has_tvars, find_tvars, etc. instead use
    a TypeConstructor as the signature of functions with {}
  . type_match only uses tvars from a given constraint environment, i.e.
    the parameters of the TypeConstructor
  * make cache_1arg an array
* finish getting rid of jl_show_to_string
* make sure inference results can be shared across instantiations of
  inner generic functions
  . when cache_method specializes a LambdaStaticData, put it on a list of
    all specializations of that LSD (like the tfunc field), and look there
    first instead of calling typeinf
  other design:
  . during specialize_ast, allocate a GF object for each inner GF, evaluate
    signatures of methods and add all the methods to the GF.
  . at runtime make an inner_gf function that calls apply_inner_gf and
    references the statically allocated GF above
    do add_method as normal on the inner GF to pick up the right set of
    methods and environments at runtime
  . in apply_inner_gf, do method lookup in the static GF object. find the
    corresponding definition in the inner GF (error if none) and call the
    looked-up function pointer with the environment from the inner GF method
    table
  . can use lambda numbering to find corresponding def
  . actually the GF allocated on first call can be used as the static one
- some kind of no-specialize hint for slots like x in
  assign(A::Array{Any}, x, i::Index) = arrayset(A,i,x)
  where we know the function is identical for all types of x
  * add x::ANY
  - reuse specialized methods for multiple types in an Any slot
* implement SubArray
- isassigned predicates
  . maybe syntax like "a?=x" or "a??"
- inlined tuple assignment optimization
  * allow inlining where formal arg appears >once if actual arg is symbol
  * in frontend add (multiple-value) hint before assigning the temp var
    for a returned tuple
* socket data corruption bug
- array primitive benchmarks
- parallelism documentation
- darray functions
  . elementwise
  . ref
  . assign
  . reduce
  . cat
ref assign reduce permute find

something about update operators, in-place operations in general

generalization of assign():
l[I] += r
l[I] = f(l[I],r)

l = update(l, x->x+r, I)

for i=I
    l[i] = f(l[i],r)
end

a := b*c
a = *(b, c, dest=a)

a:=b could be a[:,:,...]=b
maybe (a+=b) => (a=a+b) but (a[i]+=b) => (a[i]:=a[i]+b) ?

bug:
on inexact method matches, e.g. only f(Int32) exists and we don't know the
type of the argument, must merge in the result type of method_missing,
and emit a type check if we inline

-------------------------------------------------------------------------------

table updates
most general form is h[k] = f(get(h, k, default))

more efficient implementation would be:
i = findref(h,k)
putref(h,i,f(getref(h,i,default)))

-------------------------------------------------------------------------------

issues 4/16/11

clone(Array{Any,1}) matches both
clone{T}(a::Tensor{T})                      = clone(a, T, size(a))
and
clone{T}(a::Tensor{T}, dims::Size...)       = clone(a, T, dims)
but the first one should block the second one.
(fixed)

wrong:
julia> tintersect(Array{Float64,1},Tensor{Any,1})
Array{Float64,1}
(fixed)

need to distinguish covariant from invariant contexts in type intersection
we currently get this:
julia> getmethods(clone,(Array{Any,1},))
((Array{Any,1},),
 (N,1,T,T),  # ***
 AST(lambda({a::top(apply_type)(Tensor,T)},vinf(locals(),{{a,Any,false,false}},Array(Any,(0,)),()),
 ...
problem is matching Tensor{T} with Array{Any,1} yields the constraint T<:Any,
which it already is. it should yield T==Any instead.

this (and tintersect on NTuple) is needed to define Array() methods in boot.j

-------------------------------------------------------------------------------

8/23/11

- check what function vcat{T}(A::Array{T,2}...) gets specialized for on
  4 Array{Float64,2} arguments.

- make sure an argument with declared type gets the declaration recorded
  even if the argument doesn't occur in the function (but might occur in
  a nested function)

* call inference on general thunks so we don't need to make GFs for them
  in toplevel_eval

-------------------------------------------------------------------------------

counts of expression types:

static int n_expr=0;  // 252767
static int n_call=0;  // 87436
static int n_top=0;   // 31118
static int n_oth=0;   // 20502  body,locals,enter,leave
static int n_quot=0;  // 15003
                      // 10061  lambda

static int n_assi=0;  // 18926
static int n_ret=0;   // 11827
static int n_goti=0;  // 6286
static int n_goto=0;  // 6227

-------------------------------------------------------------------------------

idea for symbolic optimizations:
basically compile-time method definitions. this is a generalization of
inlining, where instead of inserting the matched function's body you insert
the result of calling an arbitrary function. normal inlining is then
equivalent to having a default version of this function that looks up and
returns the appropriate function body.

example:

staged function vcat{T<:Number}(xs::T...)
    n = length(xs)
    ar = gensym()
    inits = { :(($ar)[i] = $(xs[i])) | i=1:n }
    quote
        ($ar) = Array($T, $n)
        $(inits...)
	$ar
    end
end

-------------------------------------------------------------------------------

optimization: hoisting method lookups. if a function argument f is always
called on the same type we can hoist the lookup:

function foo(f)
  while something
    f(x)
  end
end

=>

function foo(f)
  m = getmethod(f,(typeof(x),))
  while something
    m(x)
  end
end

-------------------------------------------------------------------------------

* make getmethods return an array instead of linked tuples
* make default constructors use field types as argument types
- move more of the repl to julia

* make control-C on client while evaluating input wake up the root task
  with an InterruptException if it is waiting for something.
